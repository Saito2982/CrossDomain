import numpy as np
import math

#=======================================================================================================================
#   Module
#=======================================================================================================================

#=======================================================================================================================
#  pv_ml1
# argument : train_matrix ... user-item matrix generated by training data
#            eta0 ... learning rate with initialized value
#            u, v ... u -> user matrix, v -> item matrix
#            attribute ... the number of attribute in dataset
# return : user-item predicted matrix
# role : using SGD (Stochastic gradient decent), learning personal values-based model
# details is shown in [Shiraishi18] or graduation thesis (Yuya Shiraishi 2016)
#=======================================================================================================================\

def pv_ml1(train_matrix, eta0, u, v, attribute):
  # initialize parameter
  R = np.c_[1 + np.random.rand(attribute, attribute),  -1 - np.random.rand(attribute, attribute)]
  eta = eta0

  # shuffle training data
  random_train_data = np.random.permutation(np.transpose(train_matrix.nonzero()))

  # p : iteration count
  p = 1

  # select true-rating in shuffled training data
  for coordinate in random_train_data:
    # user
    x = coordinate[0]
    # item
    y = coordinate[1]
    # rating with user for item
    s = train_matrix[x, y]

    # initialize weight matrix R and prediction score r
    # w is elements of R
    w = np.zeros((attribute, attribute*2))
    r = 0

    # calculate predicted score
    for k in range(attribute):
      for l in range(attribute*2):
        r = r + u[x, k] * v[y, l] * R[k, l]

    # calculate gradient of w
    for i in range(attribute):
      for j in range(attribute*2):
        w[i, j] = u[x, i] * v[y, j] * ( -s + r )

    # update weight matrix
    R_old = R
    R = R_old - eta * w

    # restricted calculation
    # positive RMRate >= 0, negative RMRate < 0
    # (more detail is shown thesis)
    for i in range(attribute):
      for j in range(attribute):
        if R[i, j] <= 0:
           R[i, j] = 0.
        if R[i, j + attribute] > 0:
           R[i, j + attribute] = 0.

    # break if max of iteration count or convergence
    if p == 10000:
      break
    elif np.sum(np.sqrt(R - R_old)) == 0.01:
      break

    # update iteration count
    p = p + 1
    # update learning rate
    eta = eta0 / math.sqrt(p)

  return R

#=======================================================================================================================
#  pv_ml2
# the difference of pv_ml1, pv_ml3; elements of R have only diagonal component
# details is shown in [Shiraishi18] or graduation thesis (Yuya Shiraishi 2016)
#=======================================================================================================================\

def pv_ml2(train_matrix, eta0, u, v, attribute):
  R = np.c_[1 + np.random.rand(attribute, attribute),  -1 - np.random.rand(attribute, attribute)]
  R_left  = 1 + np.diag(np.random.rand(attribute, attribute))
  R_right = -1 -np.diag(np.random.rand(attribute, attribute))
  eta = eta0
  random_train_data = np.random.permutation(np.transpose(train_matrix.nonzero()))
  p = 1

  for coordinate in random_train_data:
    x = coordinate[0]
    y = coordinate[1]
    s = train_matrix[x, y]
    # w_left correspond to positive RMRate and w_right correspond to negative RMRate
    w_left  = np.zeros(attribute)
    w_right = np.zeros(attribute)
    r = 0

    # calculate predicted score
    for k in range(attribute):
      r = r + u[x, k] * v[y, k] * R_left[k]
      r = r + u[x, k] * v[y, k + attribute] * R_right[k]

    # calculate gradient of w
    for i in range(attribute):
      w_left[i]  = u[x, i] * v[y, i] * ( -s + r )
      w_right[i] = u[x, i] * v[y, i + attribute] * ( -s + r )

    # update weight matrix
    R_left  = R_left  - eta * w_left
    R_right = R_right - eta * w_right

    R_old = R
    R = np.c_[np.diag(R_left), np.diag(R_right)]

    # restriction
    for i in range(attribute):
      for j in range(attribute):
        if R[i, j] <= 0:
           R[i, j] = 0.
        if R[i, j + attribute] > 0:
           R[i, j + attribute] = 0.

    # convergence conditions
    if p == 10000:
      break
    elif np.sum(np.sqrt(R - R_old)) == 0.01:
      break

    # update iteration count and learning rate
    p = p + 1
    eta = eta0 / math.sqrt(p)

  return R

#=======================================================================================================================
#  pv_ml2
# the difference of pv_ml1, pv_ml2; non restriction learning
# details is shown in [Shiraishi18] or graduation thesis (Yuya Shiraishi 2016)
#=======================================================================================================================\

def pv_ml3(train_matrix, eta0, u, v, attribute):
  R = np.c_[1 + np.random.rand(attribute, attribute),   -1 + np.random.rand(attribute, attribute)]
  eta = eta0
  random_train_data = np.random.permutation(np.transpose(train_matrix.nonzero()))
  p = 1

  for coordinate in random_train_data:
    x = coordinate[0]
    y = coordinate[1]
    s = train_matrix[x, y]
    w = np.zeros((attribute, attribute*2))
    r = 0

    # calculate predicted score
    for k in range(attribute):
      for l in range(attribute*2):
        r = r + u[x, k] * v[y, l] * R[k, l]

    # calculate gradient of w
    for i in range(attribute):
      for j in range(attribute*2):
        w[i, j] = u[x, i] * v[y, j] * ( -s + r )

    # update weight matrix
    R_old = R
    R = R - eta * w

    # convergence conditions
    if p == 10000:
      break
    elif np.sum(np.sqrt(R - R_old)) == 0.01:
      break

    # update iteration count and learning rate
    p = p + 1
    eta = eta0 / math.sqrt(p)

  return R
